{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classifiaction\n",
    "\n",
    "1. **Data Preprocessing and Augmentation:**\n",
    "   - Data preprocessing is a crucial step in machine learning. In this case, images are preprocessed to make them suitable for training a deep learning model.\n",
    "   - Rescaling is used to normalize pixel values, typically scaling them to a range between 0 and 1.\n",
    "   - Augmentation techniques like rotation, shifting, shearing, zooming, and flipping are applied to artificially increase the diversity of the training dataset. This helps the model generalize better to real-world data.\n",
    "\n",
    "2. **Loading and Preprocessing Training Images:**\n",
    "   - To train a deep learning model, you need a dataset of labeled images. The `ImageDataGenerator` is used to load and preprocess these images.\n",
    "   - The directory structure should be organized with subdirectories for each class of images.\n",
    "   - `flow_from_directory` method loads images from the specified directory and performs preprocessing.\n",
    "   - `target_size` ensures all images are resized to a consistent size.\n",
    "   - `batch_size` determines how many images are processed in each training iteration.\n",
    "   - `class_mode` specifies the type of classification (e.g., binary or categorical).\n",
    "   - `shuffle` ensures that the order of images is randomized during training to prevent the model from learning the order of the dataset.\n",
    "\n",
    "3. **Creating MobileNetV2 Base Model:**\n",
    "   - MobileNetV2 is a pre-trained deep learning model that is used as a feature extractor. It's a type of convolutional neural network (CNN).\n",
    "   - The base model is used to capture the high-level features of the input images. It has been trained on a large dataset (ImageNet) and can recognize a wide range of features.\n",
    "   - `include_top=False` indicates that we're not using the final classification layer of MobileNetV2. Instead, we'll add our own custom classification layers.\n",
    "   - `weights='imagenet'` specifies that we want to use the pre-trained weights from the ImageNet dataset.\n",
    "\n",
    "4. **Adding Custom Classification Layers:**\n",
    "   - After using MobileNetV2 as a feature extractor, we add our own custom layers for classification.\n",
    "   - Global average pooling is used to reduce the spatial dimensions of the feature maps. This simplifies the model and reduces the number of parameters.\n",
    "   - Dense layers with ReLU activation are added to learn complex patterns in the feature maps.\n",
    "   - The final dense layer has as many units as there are classes in the classification task and uses a softmax activation to make class predictions.\n",
    "\n",
    "5. **Creating the Final Classification Model:**\n",
    "   - The final model is created by combining the base MobileNetV2 model with the custom classification layers.\n",
    "   - This model can take images as input and output class predictions.\n",
    "\n",
    "6. **Compiling the Model:**\n",
    "   - To train the model, it needs to be compiled with an optimizer, a loss function, and evaluation metrics.\n",
    "   - 'Adam' is a popular optimizer, and 'categorical_crossentropy' is a common loss function for multi-class classification.\n",
    "   - The 'accuracy' metric is used to evaluate how well the model is performing during training.\n",
    "\n",
    "7. **ModelCheckpoint Callback:**\n",
    "   - The `ModelCheckpoint` callback is used to save the best model during training.\n",
    "   - It monitors a specific metric, in this case, 'accuracy,' and saves the model when this metric is maximized.\n",
    "\n",
    "8. **Training the Model:**\n",
    "   - The model is trained using the training dataset and the specified configuration (e.g., number of epochs).\n",
    "   - During training, the model's weights are updated to minimize the loss function, and its performance is evaluated using the validation data.\n",
    "\n",
    "9. **Printing Training Results:**\n",
<<<<<<< HEAD
    "   - After training is complete, you can print a message to confirm the training settings used (e.g., batch size and number of epochs).\n",
    "\n",
    "### References\n",
    "[https://towardsdatascience.com/review-mobilenetv2-light-weight-model-image-classification-8febb490e61c](https://towardsdatascience.com/review-mobilenetv2-light-weight-model-image-classification-8febb490e61c) <br>\n",
    "\n",
    "[https://blog.roboflow.com/how-to-train-mobilenetv2-on-a-custom-dataset/](https://blog.roboflow.com/how-to-train-mobilenetv2-on-a-custom-dataset/) <bt>\n",
    "\n",
    "[https://github.com/EhabR98/Transfer-Learning-with-MobileNetV2/](https://github.com/EhabR98/Transfer-Learning-with-MobileNetV2/) <br>\n",
    "\n",
    "[https://paperswithcode.com/method/mobilenetv2](https://paperswithcode.com/method/mobilenetv2) <br>\n",
    "\n",
    "[https://www.youtube.com/watch?v=jztwpsIzEGc](https://www.youtube.com/watch?v=jztwpsIzEGc) <br>\n",
    "\n",
    "[https://www.kaggle.com/code/mgiraygokirmak/mobilenetv2](https://www.kaggle.com/code/mgiraygokirmak/mobilenetv2) <br>\n"
=======
    "   - After training is complete, you can print a message to confirm the training settings used (e.g., batch size and number of epochs)."
>>>>>>> 1aab59417eec0fd267a04d924a7092a0544351ad
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Store Initiate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib as plt\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Define constants\n",
    "data_dir = '../data/dataset/sample/'\n",
    "image_size = (224, 224)\n",
    "custom_class_labels = ['effective', 'mildly effective', 'not effective']\n",
    "\n",
    "\n",
    "# Create directories for training data\n",
    "train_dir = os.path.join(data_dir, 'train')\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(train_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 images belonging to 3 classes.\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\athar\\OneDrive\\Desktop\\DEV\\Adsentis\\model\\ad_classify.ipynb Cell 5\u001b[0m line \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/athar/OneDrive/Desktop/DEV/Adsentis/model/ad_classify.ipynb#X14sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m model_checkpoint \u001b[39m=\u001b[39m ModelCheckpoint(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbest_model_\u001b[39m\u001b[39m{\u001b[39;00mbest_batch_size\u001b[39m}\u001b[39;00m\u001b[39mbs_\u001b[39m\u001b[39m{\u001b[39;00mbest_epochs\u001b[39m}\u001b[39;00m\u001b[39mep.h5\u001b[39m\u001b[39m'\u001b[39m, save_best_only\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmax\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/athar/OneDrive/Desktop/DEV/Adsentis/model/ad_classify.ipynb#X14sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m \u001b[39m# Train the model using the current batch size and number of epochs\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/athar/OneDrive/Desktop/DEV/Adsentis/model/ad_classify.ipynb#X14sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(train_data, epochs\u001b[39m=\u001b[39;49mbest_epochs, callbacks\u001b[39m=\u001b[39;49m[model_checkpoint])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/athar/OneDrive/Desktop/DEV/Adsentis/model/ad_classify.ipynb#X14sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m \u001b[39m# Track the best combination\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/athar/OneDrive/Desktop/DEV/Adsentis/model/ad_classify.ipynb#X14sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m \u001b[39mif\u001b[39;00m history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m>\u001b[39m best_accuracy:\n",
      "File \u001b[1;32mc:\\Users\\athar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\athar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1734\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1735\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1736\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1739\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1740\u001b[0m ):\n\u001b[0;32m   1741\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1742\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1743\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1744\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\athar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\athar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    822\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    824\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 825\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    827\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    828\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\athar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:873\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    870\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m   \u001b[39m# This is the first call of __call__, so we have to initialize.\u001b[39;00m\n\u001b[0;32m    872\u001b[0m   initializers \u001b[39m=\u001b[39m []\n\u001b[1;32m--> 873\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_initialize(args, kwds, add_initializers_to\u001b[39m=\u001b[39;49minitializers)\n\u001b[0;32m    874\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    875\u001b[0m   \u001b[39m# At this point we know that the initialization is complete (or less\u001b[39;00m\n\u001b[0;32m    876\u001b[0m   \u001b[39m# interestingly an exception was raised) so we no longer need a lock.\u001b[39;00m\n\u001b[0;32m    877\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\athar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:694\u001b[0m, in \u001b[0;36mFunction._initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    691\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn\u001b[39m.\u001b[39m_name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    692\u001b[0m \u001b[39m# Force the definition of the function for these arguments\u001b[39;00m\n\u001b[0;32m    693\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_concrete_variable_creation_fn \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 694\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn    \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    695\u001b[0m     \u001b[39m.\u001b[39m_get_concrete_function_internal_garbage_collected(\n\u001b[0;32m    696\u001b[0m         \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds))\n\u001b[0;32m    698\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvalid_creator_scope\u001b[39m(\u001b[39m*\u001b[39munused_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39munused_kwds):\n\u001b[0;32m    699\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Disables variable creation.\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\athar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:176\u001b[0m, in \u001b[0;36mTracingCompiler._get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Returns a concrete function which cleans up its graph function.\"\"\"\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m--> 176\u001b[0m   concrete_function, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_define_concrete_function(args, kwargs)\n\u001b[0;32m    177\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\n",
      "File \u001b[1;32mc:\\Users\\athar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:171\u001b[0m, in \u001b[0;36mTracingCompiler._maybe_define_concrete_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m   args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_signature\n\u001b[0;32m    169\u001b[0m   kwargs \u001b[39m=\u001b[39m {}\n\u001b[1;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_define_function(args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\athar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:398\u001b[0m, in \u001b[0;36mTracingCompiler._maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m    395\u001b[0m args \u001b[39m=\u001b[39m placeholder_bound_args\u001b[39m.\u001b[39margs\n\u001b[0;32m    396\u001b[0m kwargs \u001b[39m=\u001b[39m placeholder_bound_args\u001b[39m.\u001b[39mkwargs\n\u001b[1;32m--> 398\u001b[0m concrete_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_concrete_function(\n\u001b[0;32m    399\u001b[0m     args, kwargs, func_graph)\n\u001b[0;32m    401\u001b[0m \u001b[39m# TODO(b/263520817): Remove access to private attribute.\u001b[39;00m\n\u001b[0;32m    402\u001b[0m graph_capture_container \u001b[39m=\u001b[39m concrete_function\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39mfunction_captures\n",
      "File \u001b[1;32mc:\\Users\\athar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:305\u001b[0m, in \u001b[0;36mTracingCompiler._create_concrete_function\u001b[1;34m(self, args, kwargs, func_graph)\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    302\u001b[0m   arg_names \u001b[39m=\u001b[39m base_arg_names\n\u001b[0;32m    304\u001b[0m concrete_function \u001b[39m=\u001b[39m monomorphic_function\u001b[39m.\u001b[39mConcreteFunction(\n\u001b[1;32m--> 305\u001b[0m     func_graph_module\u001b[39m.\u001b[39;49mfunc_graph_from_py_func(\n\u001b[0;32m    306\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_name,\n\u001b[0;32m    307\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_python_function,\n\u001b[0;32m    308\u001b[0m         args,\n\u001b[0;32m    309\u001b[0m         kwargs,\n\u001b[0;32m    310\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    311\u001b[0m         func_graph\u001b[39m=\u001b[39;49mfunc_graph,\n\u001b[0;32m    312\u001b[0m         arg_names\u001b[39m=\u001b[39;49marg_names,\n\u001b[0;32m    313\u001b[0m         capture_by_value\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_capture_by_value,\n\u001b[0;32m    314\u001b[0m         create_placeholders\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m    315\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_attributes,\n\u001b[0;32m    316\u001b[0m     spec\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_spec,\n\u001b[0;32m    317\u001b[0m     \u001b[39m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[39;00m\n\u001b[0;32m    318\u001b[0m     \u001b[39m# scope. This is not the default behavior since it gets used in some\u001b[39;00m\n\u001b[0;32m    319\u001b[0m     \u001b[39m# places (like Keras) where the FuncGraph lives longer than the\u001b[39;00m\n\u001b[0;32m    320\u001b[0m     \u001b[39m# ConcreteFunction.\u001b[39;00m\n\u001b[0;32m    321\u001b[0m     shared_func_graph\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    322\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\n",
      "File \u001b[1;32mc:\\Users\\athar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1055\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders)\u001b[0m\n\u001b[0;32m   1052\u001b[0m   \u001b[39mreturn\u001b[39;00m x\n\u001b[0;32m   1054\u001b[0m _, original_func \u001b[39m=\u001b[39m tf_decorator\u001b[39m.\u001b[39munwrap(python_func)\n\u001b[1;32m-> 1055\u001b[0m func_outputs \u001b[39m=\u001b[39m python_func(\u001b[39m*\u001b[39mfunc_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfunc_kwargs)\n\u001b[0;32m   1057\u001b[0m \u001b[39m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[0;32m   1058\u001b[0m \u001b[39m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[0;32m   1059\u001b[0m func_outputs \u001b[39m=\u001b[39m variable_utils\u001b[39m.\u001b[39mconvert_variables_to_tensors(func_outputs)\n",
      "File \u001b[1;32mc:\\Users\\athar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:597\u001b[0m, in \u001b[0;36mFunction._compiler_with_scope.<locals>.wrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    593\u001b[0m \u001b[39mwith\u001b[39;00m default_graph\u001b[39m.\u001b[39m_variable_creator_scope(scope, priority\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m):  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    594\u001b[0m   \u001b[39m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[39;00m\n\u001b[0;32m    595\u001b[0m   \u001b[39m# the function a weak reference to itself to avoid a reference cycle.\u001b[39;00m\n\u001b[0;32m    596\u001b[0m   \u001b[39mwith\u001b[39;00m OptionalXlaContext(compile_with_xla):\n\u001b[1;32m--> 597\u001b[0m     out \u001b[39m=\u001b[39m weak_wrapped_fn()\u001b[39m.\u001b[39m__wrapped__(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    598\u001b[0m   \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\athar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\autograph_util.py:41\u001b[0m, in \u001b[0;36mpy_func_from_autograph.<locals>.autograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Calls a converted version of original_func.\"\"\"\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 41\u001b[0m   \u001b[39mreturn\u001b[39;00m api\u001b[39m.\u001b[39;49mconverted_call(\n\u001b[0;32m     42\u001b[0m       original_func,\n\u001b[0;32m     43\u001b[0m       args,\n\u001b[0;32m     44\u001b[0m       kwargs,\n\u001b[0;32m     45\u001b[0m       options\u001b[39m=\u001b[39;49mconverter\u001b[39m.\u001b[39;49mConversionOptions(\n\u001b[0;32m     46\u001b[0m           recursive\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     47\u001b[0m           optional_features\u001b[39m=\u001b[39;49mautograph_options,\n\u001b[0;32m     48\u001b[0m           user_requested\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     49\u001b[0m       ))\n\u001b[0;32m     50\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m     51\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\athar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:439\u001b[0m, in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    438\u001b[0m   \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 439\u001b[0m     result \u001b[39m=\u001b[39m converted_f(\u001b[39m*\u001b[39meffective_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    440\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    441\u001b[0m     result \u001b[39m=\u001b[39m converted_f(\u001b[39m*\u001b[39meffective_args)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filek108md7f.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(step_function), (ag__\u001b[39m.\u001b[39;49mld(\u001b[39mself\u001b[39;49m), ag__\u001b[39m.\u001b[39;49mld(iterator)), \u001b[39mNone\u001b[39;49;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\athar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:377\u001b[0m, in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    374\u001b[0m   \u001b[39mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[0;32m    376\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m options\u001b[39m.\u001b[39muser_requested \u001b[39mand\u001b[39;00m conversion\u001b[39m.\u001b[39mis_allowlisted(f):\n\u001b[1;32m--> 377\u001b[0m   \u001b[39mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[0;32m    379\u001b[0m \u001b[39m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[39;00m\n\u001b[0;32m    380\u001b[0m \u001b[39m# call conversion from generated code while in nonrecursive mode. In that\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[39m# case we evidently don't want to recurse, but we still have to convert\u001b[39;00m\n\u001b[0;32m    382\u001b[0m \u001b[39m# things like builtins.\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m options\u001b[39m.\u001b[39minternal_convert_user_code:\n",
      "File \u001b[1;32mc:\\Users\\athar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:460\u001b[0m, in \u001b[0;36m_call_unconverted\u001b[1;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    459\u001b[0m   \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 460\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[1;32mc:\\Users\\athar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py:1322\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function\u001b[1;34m(model, iterator)\u001b[0m\n\u001b[0;32m   1318\u001b[0m     run_step \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mfunction(\n\u001b[0;32m   1319\u001b[0m         run_step, jit_compile\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, reduce_retracing\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   1320\u001b[0m     )\n\u001b[0;32m   1321\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(iterator)\n\u001b[1;32m-> 1322\u001b[0m outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mdistribute_strategy\u001b[39m.\u001b[39;49mrun(run_step, args\u001b[39m=\u001b[39;49m(data,))\n\u001b[0;32m   1323\u001b[0m outputs \u001b[39m=\u001b[39m reduce_per_replica(\n\u001b[0;32m   1324\u001b[0m     outputs,\n\u001b[0;32m   1325\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_strategy,\n\u001b[0;32m   1326\u001b[0m     reduction\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_reduction_method,\n\u001b[0;32m   1327\u001b[0m )\n\u001b[0;32m   1328\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\athar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1673\u001b[0m, in \u001b[0;36mStrategyBase.run\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   1668\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscope():\n\u001b[0;32m   1669\u001b[0m   \u001b[39m# tf.distribute supports Eager functions, so AutoGraph should not be\u001b[39;00m\n\u001b[0;32m   1670\u001b[0m   \u001b[39m# applied when the caller is also in Eager mode.\u001b[39;00m\n\u001b[0;32m   1671\u001b[0m   fn \u001b[39m=\u001b[39m autograph\u001b[39m.\u001b[39mtf_convert(\n\u001b[0;32m   1672\u001b[0m       fn, autograph_ctx\u001b[39m.\u001b[39mcontrol_status_ctx(), convert_by_default\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m-> 1673\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_extended\u001b[39m.\u001b[39;49mcall_for_each_replica(fn, args\u001b[39m=\u001b[39;49margs, kwargs\u001b[39m=\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\athar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3250\u001b[0m, in \u001b[0;36mStrategyExtendedV1.call_for_each_replica\u001b[1;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[0;32m   3248\u001b[0m   kwargs \u001b[39m=\u001b[39m {}\n\u001b[0;32m   3249\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_container_strategy()\u001b[39m.\u001b[39mscope():\n\u001b[1;32m-> 3250\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_for_each_replica(fn, args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\athar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:4048\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._call_for_each_replica\u001b[1;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[0;32m   4046\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call_for_each_replica\u001b[39m(\u001b[39mself\u001b[39m, fn, args, kwargs):\n\u001b[0;32m   4047\u001b[0m   \u001b[39mwith\u001b[39;00m ReplicaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_container_strategy(), replica_id_in_sync_group\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m):\n\u001b[1;32m-> 4048\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\athar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:690\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    689\u001b[0m   \u001b[39mwith\u001b[39;00m conversion_ctx:\n\u001b[1;32m--> 690\u001b[0m     \u001b[39mreturn\u001b[39;00m converted_call(f, args, kwargs, options\u001b[39m=\u001b[39;49moptions)\n\u001b[0;32m    691\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m    692\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m'\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m'\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\athar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:377\u001b[0m, in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    374\u001b[0m   \u001b[39mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[0;32m    376\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m options\u001b[39m.\u001b[39muser_requested \u001b[39mand\u001b[39;00m conversion\u001b[39m.\u001b[39mis_allowlisted(f):\n\u001b[1;32m--> 377\u001b[0m   \u001b[39mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[0;32m    379\u001b[0m \u001b[39m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[39;00m\n\u001b[0;32m    380\u001b[0m \u001b[39m# call conversion from generated code while in nonrecursive mode. In that\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[39m# case we evidently don't want to recurse, but we still have to convert\u001b[39;00m\n\u001b[0;32m    382\u001b[0m \u001b[39m# things like builtins.\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m options\u001b[39m.\u001b[39minternal_convert_user_code:\n",
      "File \u001b[1;32mc:\\Users\\athar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:459\u001b[0m, in \u001b[0;36m_call_unconverted\u001b[1;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[0;32m    456\u001b[0m   \u001b[39mreturn\u001b[39;00m f\u001b[39m.\u001b[39m\u001b[39m__self__\u001b[39m\u001b[39m.\u001b[39mcall(args, kwargs)\n\u001b[0;32m    458\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 459\u001b[0m   \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    460\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs)\n",
      "File \u001b[1;32mc:\\Users\\athar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py:1303\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function.<locals>.run_step\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_step\u001b[39m(data):\n\u001b[1;32m-> 1303\u001b[0m     outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mtrain_step(data)\n\u001b[0;32m   1304\u001b[0m     \u001b[39m# Ensure counter is updated only if `train_step` succeeds.\u001b[39;00m\n\u001b[0;32m   1305\u001b[0m     \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mcontrol_dependencies(_minimum_control_deps(outputs)):\n",
      "File \u001b[1;32mc:\\Users\\athar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py:1084\u001b[0m, in \u001b[0;36mModel.train_step\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1082\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_target_and_loss(y, loss)\n\u001b[0;32m   1083\u001b[0m \u001b[39m# Run backwards pass.\u001b[39;00m\n\u001b[1;32m-> 1084\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49mminimize(loss, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainable_variables, tape\u001b[39m=\u001b[39;49mtape)\n\u001b[0;32m   1085\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_metrics(x, y, y_pred, sample_weight)\n",
      "File \u001b[1;32mc:\\Users\\athar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\optimizers\\optimizer.py:544\u001b[0m, in \u001b[0;36m_BaseOptimizer.minimize\u001b[1;34m(self, loss, var_list, tape)\u001b[0m\n\u001b[0;32m    523\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Minimize `loss` by updating `var_list`.\u001b[39;00m\n\u001b[0;32m    524\u001b[0m \n\u001b[0;32m    525\u001b[0m \u001b[39mThis method simply computes gradient using `tf.GradientTape` and calls\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    541\u001b[0m \u001b[39m  None\u001b[39;00m\n\u001b[0;32m    542\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    543\u001b[0m grads_and_vars \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_gradients(loss, var_list, tape)\n\u001b[1;32m--> 544\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_gradients(grads_and_vars)\n",
      "File \u001b[1;32mc:\\Users\\athar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\optimizers\\optimizer.py:1230\u001b[0m, in \u001b[0;36mOptimizer.apply_gradients\u001b[1;34m(self, grads_and_vars, name, skip_gradients_aggregation, **kwargs)\u001b[0m\n\u001b[0;32m   1228\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m skip_gradients_aggregation \u001b[39mand\u001b[39;00m experimental_aggregate_gradients:\n\u001b[0;32m   1229\u001b[0m     grads_and_vars \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maggregate_gradients(grads_and_vars)\n\u001b[1;32m-> 1230\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mapply_gradients(grads_and_vars, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[1;32mc:\\Users\\athar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\optimizers\\optimizer.py:652\u001b[0m, in \u001b[0;36m_BaseOptimizer.apply_gradients\u001b[1;34m(self, grads_and_vars, name)\u001b[0m\n\u001b[0;32m    650\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply_weight_decay(trainable_variables)\n\u001b[0;32m    651\u001b[0m grads_and_vars \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(grads, trainable_variables))\n\u001b[1;32m--> 652\u001b[0m iteration \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_internal_apply_gradients(grads_and_vars)\n\u001b[0;32m    654\u001b[0m \u001b[39m# Apply variable constraints after applying gradients.\u001b[39;00m\n\u001b[0;32m    655\u001b[0m \u001b[39mfor\u001b[39;00m variable \u001b[39min\u001b[39;00m trainable_variables:\n",
      "File \u001b[1;32mc:\\Users\\athar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\optimizers\\optimizer.py:1260\u001b[0m, in \u001b[0;36mOptimizer._internal_apply_gradients\u001b[1;34m(self, grads_and_vars)\u001b[0m\n\u001b[0;32m   1256\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mesh \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_with_dtensor:\n\u001b[0;32m   1257\u001b[0m     \u001b[39m# Skip any usage of strategy logic for DTensor\u001b[39;00m\n\u001b[0;32m   1258\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m_internal_apply_gradients(grads_and_vars)\n\u001b[1;32m-> 1260\u001b[0m \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39;49m__internal__\u001b[39m.\u001b[39;49mdistribute\u001b[39m.\u001b[39;49minterim\u001b[39m.\u001b[39;49mmaybe_merge_call(\n\u001b[0;32m   1261\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_distributed_apply_gradients_fn,\n\u001b[0;32m   1262\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_distribution_strategy,\n\u001b[0;32m   1263\u001b[0m     grads_and_vars,\n\u001b[0;32m   1264\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\athar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\distribute\\merge_call_interim.py:51\u001b[0m, in \u001b[0;36mmaybe_merge_call\u001b[1;34m(fn, strategy, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Maybe invoke `fn` via `merge_call` which may or may not be fulfilled.\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \n\u001b[0;32m     33\u001b[0m \u001b[39mThe caller of this utility function requests to invoke `fn` via `merge_call`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[39m  The return value of the `fn` call.\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[39mif\u001b[39;00m strategy_supports_no_merge_call():\n\u001b[1;32m---> 51\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(strategy, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   \u001b[39mreturn\u001b[39;00m distribute_lib\u001b[39m.\u001b[39mget_replica_context()\u001b[39m.\u001b[39mmerge_call(\n\u001b[0;32m     54\u001b[0m       fn, args\u001b[39m=\u001b[39margs, kwargs\u001b[39m=\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\athar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\optimizers\\optimizer.py:1352\u001b[0m, in \u001b[0;36mOptimizer._distributed_apply_gradients_fn\u001b[1;34m(self, distribution, grads_and_vars, **kwargs)\u001b[0m\n\u001b[0;32m   1349\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_step(grad, var)\n\u001b[0;32m   1351\u001b[0m \u001b[39mfor\u001b[39;00m grad, var \u001b[39min\u001b[39;00m grads_and_vars:\n\u001b[1;32m-> 1352\u001b[0m     distribution\u001b[39m.\u001b[39;49mextended\u001b[39m.\u001b[39;49mupdate(\n\u001b[0;32m   1353\u001b[0m         var, apply_grad_to_update_var, args\u001b[39m=\u001b[39;49m(grad,), group\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[0;32m   1354\u001b[0m     )\n\u001b[0;32m   1356\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_ema:\n\u001b[0;32m   1357\u001b[0m     _, var_list \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mgrads_and_vars)\n",
      "File \u001b[1;32mc:\\Users\\athar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2994\u001b[0m, in \u001b[0;36mStrategyExtendedV2.update\u001b[1;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[0;32m   2992\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update(var, fn, args, kwargs, group)\n\u001b[0;32m   2993\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2994\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_replica_ctx_update(\n\u001b[0;32m   2995\u001b[0m       var, fn, args\u001b[39m=\u001b[39;49margs, kwargs\u001b[39m=\u001b[39;49mkwargs, group\u001b[39m=\u001b[39;49mgroup)\n",
      "File \u001b[1;32mc:\\Users\\athar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2873\u001b[0m, in \u001b[0;36mStrategyExtendedV2._replica_ctx_update\u001b[1;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[0;32m   2870\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmerge_fn\u001b[39m(_, \u001b[39m*\u001b[39mmerged_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmerged_kwargs):\n\u001b[0;32m   2871\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate(var, fn, merged_args, merged_kwargs, group\u001b[39m=\u001b[39mgroup)\n\u001b[1;32m-> 2873\u001b[0m \u001b[39mreturn\u001b[39;00m replica_context\u001b[39m.\u001b[39;49mmerge_call(merge_fn, args\u001b[39m=\u001b[39;49margs, kwargs\u001b[39m=\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\athar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3465\u001b[0m, in \u001b[0;36mReplicaContextBase.merge_call\u001b[1;34m(self, merge_fn, args, kwargs)\u001b[0m\n\u001b[0;32m   3461\u001b[0m   kwargs \u001b[39m=\u001b[39m {}\n\u001b[0;32m   3463\u001b[0m merge_fn \u001b[39m=\u001b[39m autograph\u001b[39m.\u001b[39mtf_convert(\n\u001b[0;32m   3464\u001b[0m     merge_fn, autograph_ctx\u001b[39m.\u001b[39mcontrol_status_ctx(), convert_by_default\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m-> 3465\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_merge_call(merge_fn, args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\athar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3472\u001b[0m, in \u001b[0;36mReplicaContextBase._merge_call\u001b[1;34m(self, merge_fn, args, kwargs)\u001b[0m\n\u001b[0;32m   3469\u001b[0m _push_per_thread_mode(  \u001b[39m# thread-local, so not needed with multiple threads\u001b[39;00m\n\u001b[0;32m   3470\u001b[0m     _CrossReplicaThreadMode(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_strategy))  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   3471\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3472\u001b[0m   \u001b[39mreturn\u001b[39;00m merge_fn(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_strategy, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   3473\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m   3474\u001b[0m   _pop_per_thread_mode()\n",
      "File \u001b[1;32mc:\\Users\\athar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:690\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    689\u001b[0m   \u001b[39mwith\u001b[39;00m conversion_ctx:\n\u001b[1;32m--> 690\u001b[0m     \u001b[39mreturn\u001b[39;00m converted_call(f, args, kwargs, options\u001b[39m=\u001b[39;49moptions)\n\u001b[0;32m    691\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m    692\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m'\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m'\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\athar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:377\u001b[0m, in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    374\u001b[0m   \u001b[39mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[0;32m    376\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m options\u001b[39m.\u001b[39muser_requested \u001b[39mand\u001b[39;00m conversion\u001b[39m.\u001b[39mis_allowlisted(f):\n\u001b[1;32m--> 377\u001b[0m   \u001b[39mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[0;32m    379\u001b[0m \u001b[39m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[39;00m\n\u001b[0;32m    380\u001b[0m \u001b[39m# call conversion from generated code while in nonrecursive mode. In that\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[39m# case we evidently don't want to recurse, but we still have to convert\u001b[39;00m\n\u001b[0;32m    382\u001b[0m \u001b[39m# things like builtins.\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m options\u001b[39m.\u001b[39minternal_convert_user_code:\n",
      "File \u001b[1;32mc:\\Users\\athar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:459\u001b[0m, in \u001b[0;36m_call_unconverted\u001b[1;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[0;32m    456\u001b[0m   \u001b[39mreturn\u001b[39;00m f\u001b[39m.\u001b[39m\u001b[39m__self__\u001b[39m\u001b[39m.\u001b[39mcall(args, kwargs)\n\u001b[0;32m    458\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 459\u001b[0m   \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    460\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs)\n",
      "File \u001b[1;32mc:\\Users\\athar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2871\u001b[0m, in \u001b[0;36mStrategyExtendedV2._replica_ctx_update.<locals>.merge_fn\u001b[1;34m(_, *merged_args, **merged_kwargs)\u001b[0m\n\u001b[0;32m   2870\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmerge_fn\u001b[39m(_, \u001b[39m*\u001b[39mmerged_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmerged_kwargs):\n\u001b[1;32m-> 2871\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupdate(var, fn, merged_args, merged_kwargs, group\u001b[39m=\u001b[39;49mgroup)\n",
      "File \u001b[1;32mc:\\Users\\athar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2992\u001b[0m, in \u001b[0;36mStrategyExtendedV2.update\u001b[1;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[0;32m   2989\u001b[0m   fn \u001b[39m=\u001b[39m autograph\u001b[39m.\u001b[39mtf_convert(\n\u001b[0;32m   2990\u001b[0m       fn, autograph_ctx\u001b[39m.\u001b[39mcontrol_status_ctx(), convert_by_default\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m   2991\u001b[0m   \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_container_strategy()\u001b[39m.\u001b[39mscope():\n\u001b[1;32m-> 2992\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update(var, fn, args, kwargs, group)\n\u001b[0;32m   2993\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2994\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_replica_ctx_update(\n\u001b[0;32m   2995\u001b[0m       var, fn, args\u001b[39m=\u001b[39margs, kwargs\u001b[39m=\u001b[39mkwargs, group\u001b[39m=\u001b[39mgroup)\n",
      "File \u001b[1;32mc:\\Users\\athar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:4062\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._update\u001b[1;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[0;32m   4059\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_update\u001b[39m(\u001b[39mself\u001b[39m, var, fn, args, kwargs, group):\n\u001b[0;32m   4060\u001b[0m   \u001b[39m# The implementations of _update() and _update_non_slot() are identical\u001b[39;00m\n\u001b[0;32m   4061\u001b[0m   \u001b[39m# except _update() passes `var` as the first argument to `fn()`.\u001b[39;00m\n\u001b[1;32m-> 4062\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_non_slot(var, fn, (var,) \u001b[39m+\u001b[39;49m \u001b[39mtuple\u001b[39;49m(args), kwargs, group)\n",
      "File \u001b[1;32mc:\\Users\\athar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:4068\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._update_non_slot\u001b[1;34m(self, colocate_with, fn, args, kwargs, should_group)\u001b[0m\n\u001b[0;32m   4064\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_update_non_slot\u001b[39m(\u001b[39mself\u001b[39m, colocate_with, fn, args, kwargs, should_group):\n\u001b[0;32m   4065\u001b[0m   \u001b[39m# TODO(josh11b): Figure out what we should be passing to UpdateContext()\u001b[39;00m\n\u001b[0;32m   4066\u001b[0m   \u001b[39m# once that value is used for something.\u001b[39;00m\n\u001b[0;32m   4067\u001b[0m   \u001b[39mwith\u001b[39;00m UpdateContext(colocate_with):\n\u001b[1;32m-> 4068\u001b[0m     result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   4069\u001b[0m     \u001b[39mif\u001b[39;00m should_group:\n\u001b[0;32m   4070\u001b[0m       \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\athar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:690\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    689\u001b[0m   \u001b[39mwith\u001b[39;00m conversion_ctx:\n\u001b[1;32m--> 690\u001b[0m     \u001b[39mreturn\u001b[39;00m converted_call(f, args, kwargs, options\u001b[39m=\u001b[39;49moptions)\n\u001b[0;32m    691\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m    692\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m'\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m'\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\athar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:331\u001b[0m, in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[39mif\u001b[39;00m conversion\u001b[39m.\u001b[39mis_in_allowlist_cache(f, options):\n\u001b[0;32m    330\u001b[0m   logging\u001b[39m.\u001b[39mlog(\u001b[39m2\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mAllowlisted \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m: from cache\u001b[39m\u001b[39m'\u001b[39m, f)\n\u001b[1;32m--> 331\u001b[0m   \u001b[39mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options, \u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    333\u001b[0m \u001b[39mif\u001b[39;00m ag_ctx\u001b[39m.\u001b[39mcontrol_status_ctx()\u001b[39m.\u001b[39mstatus \u001b[39m==\u001b[39m ag_ctx\u001b[39m.\u001b[39mStatus\u001b[39m.\u001b[39mDISABLED:\n\u001b[0;32m    334\u001b[0m   logging\u001b[39m.\u001b[39mlog(\u001b[39m2\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mAllowlisted: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m: AutoGraph is disabled in context\u001b[39m\u001b[39m'\u001b[39m, f)\n",
      "File \u001b[1;32mc:\\Users\\athar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:459\u001b[0m, in \u001b[0;36m_call_unconverted\u001b[1;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[0;32m    456\u001b[0m   \u001b[39mreturn\u001b[39;00m f\u001b[39m.\u001b[39m\u001b[39m__self__\u001b[39m\u001b[39m.\u001b[39mcall(args, kwargs)\n\u001b[0;32m    458\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 459\u001b[0m   \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    460\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs)\n",
      "File \u001b[1;32mc:\\Users\\athar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\optimizers\\optimizer.py:1349\u001b[0m, in \u001b[0;36mOptimizer._distributed_apply_gradients_fn.<locals>.apply_grad_to_update_var\u001b[1;34m(var, grad)\u001b[0m\n\u001b[0;32m   1347\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_step_xla(grad, var, \u001b[39mid\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_var_key(var)))\n\u001b[0;32m   1348\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1349\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_step(grad, var)\n",
      "File \u001b[1;32mc:\\Users\\athar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\optimizers\\optimizer.py:241\u001b[0m, in \u001b[0;36m_BaseOptimizer._update_step\u001b[1;34m(self, gradient, variable)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_var_key(variable) \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_dict:\n\u001b[0;32m    233\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\n\u001b[0;32m    234\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe optimizer cannot recognize variable \u001b[39m\u001b[39m{\u001b[39;00mvariable\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    235\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThis usually means you are trying to call the optimizer to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    239\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`tf.keras.optimizers.legacy.\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    240\u001b[0m     )\n\u001b[1;32m--> 241\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupdate_step(gradient, variable)\n",
      "File \u001b[1;32mc:\\Users\\athar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\optimizers\\adam.py:204\u001b[0m, in \u001b[0;36mAdam.update_step\u001b[1;34m(self, gradient, variable)\u001b[0m\n\u001b[0;32m    202\u001b[0m     v_hat\u001b[39m.\u001b[39massign(tf\u001b[39m.\u001b[39mmaximum(v_hat, v))\n\u001b[0;32m    203\u001b[0m     v \u001b[39m=\u001b[39m v_hat\n\u001b[1;32m--> 204\u001b[0m variable\u001b[39m.\u001b[39massign_sub((m \u001b[39m*\u001b[39;49m alpha) \u001b[39m/\u001b[39m (tf\u001b[39m.\u001b[39msqrt(v) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepsilon))\n",
      "File \u001b[1;32mc:\\Users\\athar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py:1011\u001b[0m, in \u001b[0;36mVariable._OverloadOperator.<locals>._run_op\u001b[1;34m(a, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1009\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_op\u001b[39m(a, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m   1010\u001b[0m   \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m-> 1011\u001b[0m   \u001b[39mreturn\u001b[39;00m tensor_oper(a\u001b[39m.\u001b[39;49mvalue(), \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\athar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:633\u001b[0m, in \u001b[0;36mBaseResourceVariable.value\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cached_value\n\u001b[0;32m    632\u001b[0m \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mcolocate_with(\u001b[39mNone\u001b[39;00m, ignore_existing\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m--> 633\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_variable_op()\n",
      "File \u001b[1;32mc:\\Users\\athar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:794\u001b[0m, in \u001b[0;36mBaseResourceVariable._read_variable_op\u001b[1;34m(self, no_copy)\u001b[0m\n\u001b[0;32m    792\u001b[0m       result \u001b[39m=\u001b[39m read_and_set_handle(no_copy)\n\u001b[0;32m    793\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 794\u001b[0m   result \u001b[39m=\u001b[39m read_and_set_handle(no_copy)\n\u001b[0;32m    796\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m    797\u001b[0m   \u001b[39m# Note that if a control flow context is active the input of the read op\u001b[39;00m\n\u001b[0;32m    798\u001b[0m   \u001b[39m# might not actually be the handle. This line bypasses it.\u001b[39;00m\n\u001b[0;32m    799\u001b[0m   record\u001b[39m.\u001b[39mrecord_operation(\n\u001b[0;32m    800\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mReadVariableOp\u001b[39m\u001b[39m\"\u001b[39m, [result], [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle],\n\u001b[0;32m    801\u001b[0m       backward_function\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: [x],\n\u001b[0;32m    802\u001b[0m       forward_function\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: [x])\n",
      "File \u001b[1;32mc:\\Users\\athar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:784\u001b[0m, in \u001b[0;36mBaseResourceVariable._read_variable_op.<locals>.read_and_set_handle\u001b[1;34m(no_copy)\u001b[0m\n\u001b[0;32m    782\u001b[0m \u001b[39mif\u001b[39;00m no_copy \u001b[39mand\u001b[39;00m forward_compat\u001b[39m.\u001b[39mforward_compatible(\u001b[39m2022\u001b[39m, \u001b[39m5\u001b[39m, \u001b[39m3\u001b[39m):\n\u001b[0;32m    783\u001b[0m   gen_resource_variable_ops\u001b[39m.\u001b[39mdisable_copy_on_read(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle)\n\u001b[1;32m--> 784\u001b[0m result \u001b[39m=\u001b[39m gen_resource_variable_ops\u001b[39m.\u001b[39;49mread_variable_op(\n\u001b[0;32m    785\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dtype)\n\u001b[0;32m    786\u001b[0m _maybe_set_handle_data(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dtype, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle, result)\n\u001b[0;32m    787\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\athar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\gen_resource_variable_ops.py:595\u001b[0m, in \u001b[0;36mread_variable_op\u001b[1;34m(resource, dtype, name)\u001b[0m\n\u001b[0;32m    593\u001b[0m \u001b[39m# Add nodes to the TensorFlow graph.\u001b[39;00m\n\u001b[0;32m    594\u001b[0m dtype \u001b[39m=\u001b[39m _execute\u001b[39m.\u001b[39mmake_type(dtype, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 595\u001b[0m _, _, _op, _outputs \u001b[39m=\u001b[39m _op_def_library\u001b[39m.\u001b[39;49m_apply_op_helper(\n\u001b[0;32m    596\u001b[0m       \u001b[39m\"\u001b[39;49m\u001b[39mReadVariableOp\u001b[39;49m\u001b[39m\"\u001b[39;49m, resource\u001b[39m=\u001b[39;49mresource, dtype\u001b[39m=\u001b[39;49mdtype, name\u001b[39m=\u001b[39;49mname)\n\u001b[0;32m    597\u001b[0m _result \u001b[39m=\u001b[39m _outputs[:]\n\u001b[0;32m    598\u001b[0m \u001b[39mif\u001b[39;00m _execute\u001b[39m.\u001b[39mmust_record_gradient():\n",
      "File \u001b[1;32mc:\\Users\\athar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:795\u001b[0m, in \u001b[0;36m_apply_op_helper\u001b[1;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    790\u001b[0m must_colocate_inputs \u001b[39m=\u001b[39m [val \u001b[39mfor\u001b[39;00m arg, val \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(op_def\u001b[39m.\u001b[39minput_arg, inputs)\n\u001b[0;32m    791\u001b[0m                         \u001b[39mif\u001b[39;00m arg\u001b[39m.\u001b[39mis_ref]\n\u001b[0;32m    792\u001b[0m \u001b[39mwith\u001b[39;00m _MaybeColocateWith(must_colocate_inputs):\n\u001b[0;32m    793\u001b[0m   \u001b[39m# Add Op to graph\u001b[39;00m\n\u001b[0;32m    794\u001b[0m   \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m--> 795\u001b[0m   op \u001b[39m=\u001b[39m g\u001b[39m.\u001b[39;49m_create_op_internal(op_type_name, inputs, dtypes\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    796\u001b[0m                              name\u001b[39m=\u001b[39;49mscope, input_types\u001b[39m=\u001b[39;49minput_types,\n\u001b[0;32m    797\u001b[0m                              attrs\u001b[39m=\u001b[39;49mattr_protos, op_def\u001b[39m=\u001b[39;49mop_def)\n\u001b[0;32m    799\u001b[0m \u001b[39m# `outputs` is returned as a separate return value so that the output\u001b[39;00m\n\u001b[0;32m    800\u001b[0m \u001b[39m# tensors can the `op` per se can be decoupled so that the\u001b[39;00m\n\u001b[0;32m    801\u001b[0m \u001b[39m# `op_callbacks` can function properly. See framework/op_callbacks.py\u001b[39;00m\n\u001b[0;32m    802\u001b[0m \u001b[39m# for more details.\u001b[39;00m\n\u001b[0;32m    803\u001b[0m outputs \u001b[39m=\u001b[39m op\u001b[39m.\u001b[39moutputs\n",
      "File \u001b[1;32mc:\\Users\\athar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:670\u001b[0m, in \u001b[0;36mFuncGraph._create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m    668\u001b[0m   inp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcapture(inp)\n\u001b[0;32m    669\u001b[0m   captured_inputs\u001b[39m.\u001b[39mappend(inp)\n\u001b[1;32m--> 670\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_create_op_internal(  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    671\u001b[0m     op_type, captured_inputs, dtypes, input_types, name, attrs, op_def,\n\u001b[0;32m    672\u001b[0m     compute_device)\n",
      "File \u001b[1;32mc:\\Users\\athar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:3381\u001b[0m, in \u001b[0;36mGraph._create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m   3378\u001b[0m \u001b[39m# _create_op_helper mutates the new Operation. `_mutation_lock` ensures a\u001b[39;00m\n\u001b[0;32m   3379\u001b[0m \u001b[39m# Session.run call cannot occur between creating and mutating the op.\u001b[39;00m\n\u001b[0;32m   3380\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mutation_lock():\n\u001b[1;32m-> 3381\u001b[0m   ret \u001b[39m=\u001b[39m Operation\u001b[39m.\u001b[39;49mfrom_node_def(\n\u001b[0;32m   3382\u001b[0m       node_def,\n\u001b[0;32m   3383\u001b[0m       \u001b[39mself\u001b[39;49m,\n\u001b[0;32m   3384\u001b[0m       inputs\u001b[39m=\u001b[39;49minputs,\n\u001b[0;32m   3385\u001b[0m       output_types\u001b[39m=\u001b[39;49mdtypes,\n\u001b[0;32m   3386\u001b[0m       control_inputs\u001b[39m=\u001b[39;49mcontrol_inputs,\n\u001b[0;32m   3387\u001b[0m       input_types\u001b[39m=\u001b[39;49minput_types,\n\u001b[0;32m   3388\u001b[0m       original_op\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_default_original_op,\n\u001b[0;32m   3389\u001b[0m       op_def\u001b[39m=\u001b[39;49mop_def,\n\u001b[0;32m   3390\u001b[0m   )\n\u001b[0;32m   3391\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_op_helper(ret, compute_device\u001b[39m=\u001b[39mcompute_device)\n\u001b[0;32m   3392\u001b[0m \u001b[39mreturn\u001b[39;00m ret\n",
      "File \u001b[1;32mc:\\Users\\athar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1889\u001b[0m, in \u001b[0;36mOperation.from_node_def\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   1886\u001b[0m     control_input_ops\u001b[39m.\u001b[39mappend(control_op)\n\u001b[0;32m   1888\u001b[0m \u001b[39m# Initialize c_op from node_def and other inputs\u001b[39;00m\n\u001b[1;32m-> 1889\u001b[0m c_op \u001b[39m=\u001b[39m _create_c_op(g, node_def, inputs, control_input_ops, op_def\u001b[39m=\u001b[39;49mop_def)\n\u001b[0;32m   1890\u001b[0m \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m Operation(c_op, GraphTensor)\n\u001b[0;32m   1891\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init(g)\n",
      "File \u001b[1;32mc:\\Users\\athar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\athar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1721\u001b[0m, in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs, op_def, extract_traceback)\u001b[0m\n\u001b[0;32m   1719\u001b[0m \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1720\u001b[0m \u001b[39mwith\u001b[39;00m graph\u001b[39m.\u001b[39m_c_graph\u001b[39m.\u001b[39mget() \u001b[39mas\u001b[39;00m c_graph:\n\u001b[1;32m-> 1721\u001b[0m   op_desc \u001b[39m=\u001b[39m pywrap_tf_session\u001b[39m.\u001b[39;49mTF_NewOperation(c_graph,\n\u001b[0;32m   1722\u001b[0m                                               compat\u001b[39m.\u001b[39;49mas_str(node_def\u001b[39m.\u001b[39;49mop),\n\u001b[0;32m   1723\u001b[0m                                               compat\u001b[39m.\u001b[39;49mas_str(node_def\u001b[39m.\u001b[39;49mname))\n\u001b[0;32m   1724\u001b[0m \u001b[39mif\u001b[39;00m node_def\u001b[39m.\u001b[39mdevice:\n\u001b[0;32m   1725\u001b[0m   pywrap_tf_session\u001b[39m.\u001b[39mTF_SetDevice(op_desc, compat\u001b[39m.\u001b[39mas_str(node_def\u001b[39m.\u001b[39mdevice))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
<<<<<<< HEAD
    "# Specify the GPU device to use, if available\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # Allow GPU memory growth to prevent resource contention\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)])  # Adjust memory limit as needed\n",
    "else:\n",
    "    print(\"No GPU devices found. Training on CPU.\")\n",
    "\n",
=======
>>>>>>> 1aab59417eec0fd267a04d924a7092a0544351ad
    "# Define constants\n",
    "data_dir = '../data/dataset/sample/'\n",
    "image_size = (224, 224)\n",
    "custom_class_labels = ['effective', 'mildly effective', 'not effective']\n",
    "\n",
    "# Batch sizes and epochs to iterate through\n",
    "batch_sizes = [32, 64, 128]  # You can adjust the batch sizes here\n",
    "epochs_list = [5, 10, 15]  # You can adjust the number of epochs here\n",
    "\n",
    "best_accuracy = 0.0  # Variable to store the best accuracy\n",
    "best_batch_size = 0\n",
    "best_epochs = 0\n",
    "\n",
    "for best_batch_size in batch_sizes:\n",
    "    for best_epochs in epochs_list:\n",
    "        # Data preprocessing and augmentation\n",
    "        datagen = ImageDataGenerator(\n",
    "            rescale=1.0 / 255,\n",
    "            rotation_range=20,\n",
    "            width_shift_range=0.2,\n",
    "            height_shift_range=0.2,\n",
    "            shear_range=0.2,\n",
    "            zoom_range=0.2,\n",
    "            horizontal_flip=True,\n",
    "            fill_mode='nearest'\n",
    "        )\n",
    "\n",
    "        # Load and preprocess images with custom class labels for training\n",
    "        train_data = datagen.flow_from_directory(\n",
    "            train_dir,\n",
    "            target_size=image_size,\n",
    "            batch_size=best_batch_size,\n",
    "            class_mode='categorical',\n",
    "            classes=custom_class_labels,\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        # Create MobileNetV2 base model with pre-trained weights\n",
    "        base_model = MobileNetV2(input_shape=(image_size[0], image_size[1], 3), include_top=False, weights='imagenet')\n",
    "\n",
    "        # Add custom classification layers on top of the base model\n",
    "        x = GlobalAveragePooling2D()(base_model.output)\n",
    "        x = Dense(1024, activation='relu')(x)\n",
    "        predictions = Dense(len(custom_class_labels), activation='softmax')(x)\n",
    "\n",
    "        # Create the final classification model\n",
    "        model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        # Define a ModelCheckpoint callback to save the best model based on training accuracy\n",
    "        model_checkpoint = ModelCheckpoint(f'best_model_{best_batch_size}bs_{best_epochs}ep.h5', save_best_only=True, monitor='accuracy', mode='max')\n",
    "\n",
    "        # Train the model using the current batch size and number of epochs\n",
    "        history = model.fit(train_data, epochs=best_epochs, callbacks=[model_checkpoint])\n",
    "\n",
    "        # Track the best combination\n",
    "        if history.history['accuracy'][-1] > best_accuracy:\n",
    "            best_accuracy = history.history['accuracy'][-1]\n",
    "            best_batch_size = best_batch_size\n",
    "            best_epochs = best_epochs\n",
    "\n",
    "# Print the best combination\n",
    "print(f\"Best Batch Size: {best_batch_size}\")\n",
    "print(f\"Best Number of Epochs: {best_epochs}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model: Batch Size - 32, Epochs - 10\n"
     ]
    }
   ],
   "source": [
    "# Find the best model based on validation accuracy\n",
    "print(f\"Best Model: Batch Size - {best_batch_size}, Epochs - {best_epochs}, Acc - {best_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 images belonging to 3 classes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 19s 19s/step - loss: 1.3389 - accuracy: 0.6667\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 1s 845ms/step - loss: 0.0289 - accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.9657e-04 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 1s 1s/step - loss: 9.6428e-05 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.9471e-06 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8147e-06 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 1s 998ms/step - loss: 6.6751e-05 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.7404e-05 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.3047e-06 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.9868e-07 - accuracy: 1.0000\n",
      "Training completed with 32 batch size and 10 epochs.\n"
     ]
    }
   ],
   "source": [
    "# Data preprocessing and augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Load and preprocess images with custom class labels for training\n",
    "train_data = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=best_batch_size,\n",
    "    class_mode='categorical',\n",
    "    classes=custom_class_labels,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Create MobileNetV2 base model with pre-trained weights\n",
    "base_model = MobileNetV2(input_shape=(image_size[0], image_size[1], 3), include_top=False, weights='imagenet')\n",
    "\n",
    "# Add custom classification layers on top of the base model\n",
    "x = GlobalAveragePooling2D()(base_model.output)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(len(custom_class_labels), activation='softmax')(x)\n",
    "\n",
    "# Create the final classification model\n",
    "best_model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compile the best model\n",
    "best_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the best model\n",
    "history = best_model.fit(train_data, epochs=best_epochs)\n",
    "\n",
    "# Print the training results\n",
    "print(f\"Training completed with {best_batch_size} batch size and {best_epochs} epochs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training accuracy and loss\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Custom model name\n",
    "model_name = \"custom_sentiment_model\"\n",
    "\n",
    "# Save the model with a custom name\n",
    "model.save(f'../data/model/Ads_Sentt_{best_batch_size}bs_{best_epochs}ep.keras')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the saved model\n",
    "model = load_model('../data/model/Ads_Sentt_32bs_10ep.keras')\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 51,
=======
   "execution_count": 48,
>>>>>>> 1aab59417eec0fd267a04d924a7092a0544351ad
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "1/1 [==============================] - 0s 208ms/step\n",
=======
      "1/1 [==============================] - 0s 193ms/step\n",
>>>>>>> 1aab59417eec0fd267a04d924a7092a0544351ad
      "Predicted class label: effective\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "\n",
    "# Define the class labels\n",
    "custom_class_labels = ['effective', 'mildly effective', 'not effective']\n",
    "\n",
    "# Load and preprocess an image\n",
<<<<<<< HEAD
    "img_path = '../data/dataset/sample/validation/effective/Coke.jpg'\n",
=======
    "img_path = '../data/dataset/sample/validation/effective/mcd.jpg'\n",
>>>>>>> 1aab59417eec0fd267a04d924a7092a0544351ad
    "img = image.load_img(img_path, target_size=(224, 224))  # Adjust target size as needed\n",
    "img_array = image.img_to_array(img)\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "img_array /= 255.0  # Normalize the image\n",
    "\n",
    "# Make a prediction\n",
    "predictions = model.predict(img_array)\n",
    "\n",
    "# Get the predicted class label\n",
    "predicted_class_index = np.argmax(predictions)\n",
    "predicted_class_label = custom_class_labels[predicted_class_index]\n",
    "\n",
    "print(f\"Predicted class label: {predicted_class_label}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
